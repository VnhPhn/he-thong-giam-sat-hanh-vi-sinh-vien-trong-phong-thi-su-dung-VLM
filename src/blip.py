import os
import cv2
import torch
torch.backends.cudnn.benchmark = True
import gradio as gr
import numpy as np
from ultralytics import YOLO
from PIL import Image
from transformers import BlipForQuestionAnswering, BlipProcessor

# ========================
# ‚öôÔ∏è C·∫§U H√åNH
# ========================
PROJECT_ROOT = r"D:\DaihocDaiNam\Nam4\Ki1\Chuyen_doi_so\Baitaplon\exam_monitoring_vlm"
os.chdir(PROJECT_ROOT)
device = "cuda" if torch.cuda.is_available() else "cpu"

print("üîπ Loading YOLO...")
yolo = YOLO(r"runs\detect\train_rtx3050\weights\best.pt")
print("‚úÖ YOLO ready.")

print("üîπ Loading BLIP VQA...")
blip_model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)
blip_proc = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
print(f"‚úÖ BLIP ready on {device}")

# ========================
# üß† H√ÄM PH√ÇN T√çCH ·∫¢NH
# ========================
def analyze_image(image, question):
    if image is None:
        return None, "‚ö†Ô∏è H√£y t·∫£i l√™n ·∫£nh tr∆∞·ªõc."

    # Chuy·ªÉn ·∫£nh sang OpenCV
    if isinstance(image, str):
        img_cv = cv2.imread(image)
    else:
        img_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)

    h, w, _ = img_cv.shape
   # YOLO detect m·∫°nh h∆°n
    results = yolo.predict(
    source=img_cv,
    conf=0.6,
    iou=0.5,
    imgsz=640,
    device=device,
    verbose=False
    )[0]


    cheat_boxes = []
    cheat_index = 1

    # V·∫Ω box & x√°c ƒë·ªãnh v·ªã tr√≠
    for box in results.boxes:
        cls = int(box.cls[0])
        label = yolo.names.get(cls, str(cls))
        conf = float(box.conf[0])
        x1, y1, x2, y2 = map(int, box.xyxy[0])
        cx = (x1 + x2) // 2
        cy = (y1 + y2) // 2

        # V·ªã tr√≠ t∆∞∆°ng ƒë·ªëi trong ·∫£nh
        pos_x = "b√™n tr√°i" if cx < w/3 else "gi·ªØa" if cx < 2*w/3 else "b√™n ph·∫£i"
        pos_y = "h√†ng ƒë·∫ßu" if cy < h/3 else "h√†ng gi·ªØa" if cy < 2*h/3 else "h√†ng sau"

        # V·∫Ω m√†u kh√°c nhau cho t·ª´ng nh√£n
        if label == "cheating":
            color = (0, 0, 255)
            tag = f"cheating-{cheat_index}"
            cheat_boxes.append((pos_x, pos_y, tag))
            cheat_index += 1
        else:
            color = (0, 255, 0)
            tag = "non-cheating"

        # V·∫Ω khung & nh√£n
        cv2.rectangle(img_cv, (x1, y1), (x2, y2), color, 2)
        cv2.putText(img_cv, tag, (x1, y1 - 8),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # -------------------
    # 1Ô∏è‚É£ Tr·∫£ l·ªùi b·∫±ng YOLO n·∫øu h·ªèi gian l·∫≠n
    # -------------------
    lower_q = question.lower()
    if any(k in lower_q for k in ["gian l·∫≠n", "cheat", "ƒëi·ªán tho·∫°i", "phone"]):
        if not cheat_boxes:
            answer = "Kh√¥ng c√≥ ai gian l·∫≠n ho·∫∑c d√πng ƒëi·ªán tho·∫°i."
        else:
            descs = [
                f"{i+1}. {tag} ·ªü {y} {x}."
                for i, (x, y, tag) in enumerate(cheat_boxes)
            ]
            answer = f"C√≥ {len(cheat_boxes)} ng∆∞·ªùi ƒëang gian l·∫≠n:\n" + "\n".join(descs)
        return img_cv, answer

    # -------------------
    # 2Ô∏è‚É£ BLIP tr·∫£ l·ªùi c√¢u h·ªèi chung
    # -------------------
    img_pil = Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    inputs = blip_proc(img_pil, question, return_tensors="pt").to(device)
    out = blip_model.generate(**inputs, max_new_tokens=30)
    answer = blip_proc.decode(out[0], skip_special_tokens=True)
    return img_cv, answer


# ========================
# üí¨ H√ÄM CHAT
# ========================
def chat_with_vlm(message, history, image):
    if image is None:
        return "‚ö†Ô∏è H√£y t·∫£i l√™n ·∫£nh l·ªõp h·ªçc tr∆∞·ªõc."
    annotated, answer = analyze_image(image, message)
    cv2.imwrite("output_temp.jpg", annotated)
    return f"üß† **Answer:** {answer}"

# ========================
# üåê GIAO DI·ªÜN
# ========================
with gr.Blocks(theme="soft") as demo:
    gr.Markdown("## üß† Chat v·ªõi AI gi√°m s√°t thi c·ª≠ ‚Äî YOLOv8 + BLIP-VQA (v3)")
    gr.Markdown("T·∫£i ·∫£nh ph√≤ng thi ‚Üí h·ªèi: **Ai ƒëang gian l·∫≠n?**, **Ng∆∞·ªùi gian l·∫≠n ·ªü ƒë√¢u?**, ho·∫∑c **Who is using the phone?**")

    with gr.Row():
        img_input = gr.Image(
        label="üì∑ Upload Exam Image",
        type="pil",
        image_mode="RGB",
        streaming=False,
        height=None,
        width=None,
        )

        chatbot = gr.ChatInterface(
            fn=lambda message, history, image: chat_with_vlm(message, history, image),
            additional_inputs=[img_input],
            textbox=gr.Textbox(placeholder="Nh·∫≠p c√¢u h·ªèi...", scale=4),
        )

if __name__ == "__main__":
    demo.launch(share=False)
