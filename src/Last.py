# exam_guard.py
import os
import time
import gc
import math
import threading
from datetime import datetime

import cv2
import torch
import numpy as np
from PIL import Image
from ultralytics import YOLO
from transformers import Blip2Processor, Blip2ForConditionalGeneration
import gradio as gr

# ====== (Windows) C·∫£nh b√°o √¢m thanh
try:
    import winsound
    def beep(): winsound.Beep(2000, 700)
except Exception:
    def beep(): pass  # no-op tr√™n non-Windows

# ========================
# ‚öôÔ∏è C·∫§U H√åNH
# ========================
CAM_URL = "http://172.16.15.0:4747/video"     # üëâ IP Webcam c·ªßa b·∫°n (DroidCam/IP Webcam)
YOLO_WEIGHTS = r"runs/detect/train_rtx3050/weights/best.pt"  # ho·∫∑c yolov8s.pt ƒë·ªÉ test
CONF_DET = 0.01          # Ng∆∞·ª°ng YOLO
BLIP_MAX_NEW_TOKENS = 32 # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi
EVIDENCE_DIR = "logs/evidence"
LOG_FILE = "logs/evidence_log.txt"
os.makedirs(EVIDENCE_DIR, exist_ok=True)
os.makedirs("logs", exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float16 if DEVICE == "cuda" else torch.float32
print(f"üß† Thi·∫øt b·ªã: {DEVICE}, dtype: {DTYPE}")

# ========================
# üîπ T·∫¢I M√î H√åNH
# ========================
print("üîπ ƒêang t·∫£i YOLO...")
yolo = YOLO(YOLO_WEIGHTS)
yolo.to(DEVICE)
print("‚úÖ YOLO s·∫µn s√†ng.")

print("üîπ ƒêang t·∫£i BLIP-2 Flan-T5-XL...")
blip_proc = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
blip_model = Blip2ForConditionalGeneration.from_pretrained(
    "Salesforce/blip2-flan-t5-xl",
    torch_dtype=DTYPE
).to(DEVICE)
blip_model.eval()
print("‚úÖ BLIP-2 Flan-T5-XL s·∫µn s√†ng.")

# ========================
# üî§ TI·ªÜN √çCH & NG∆Ø·ª†NG
# ========================
SUS_KEYWORDS = [
    "cheating", "copying", "looking at another paper",
    "using a phone", "phone", "mobile", "device", "texting",
    "peeking", "whispering", "passing paper"
]

# C√°c t√™n l·ªõp kh·∫£ d·ª•ng t√πy b·ªô d·ªØ li·ªáu c·ªßa b·∫°n
# B·∫°n c√≥ th·ªÉ s·ª≠a l·∫°i cho tr√πng kh·ªõp dataset
CHEAT_LABELS = {"cheating"}         # N·∫øu m√¥ h√¨nh c√≥ l·ªõp 'cheating'
PERSON_LABELS = {"person", "student", "pupil"}
PHONE_LABELS = {"phone", "cell phone", "mobile", "smartphone"}

def iou(a, b):
    # a, b: [x1, y1, x2, y2]
    ax1, ay1, ax2, ay2 = a
    bx1, by1, bx2, by2 = b
    inter_x1, inter_y1 = max(ax1, bx1), max(ay1, by1)
    inter_x2, inter_y2 = min(ax2, bx2), min(ay2, by2)
    inter_w, inter_h = max(0, inter_x2 - inter_x1), max(0, inter_y2 - inter_y1)
    inter = inter_w * inter_h
    area_a = (ax2-ax1)*(ay2-ay1)
    area_b = (bx2-bx1)*(by2-by1)
    union = area_a + area_b - inter + 1e-6
    return inter / union

def expand_box(x1, y1, x2, y2, scale, w, h):
    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
    bw, bh = (x2 - x1) * scale, (y2 - y1) * scale
    nx1, ny1 = int(max(0, cx - bw/2)), int(max(0, cy - bh/2))
    nx2, ny2 = int(min(w-1, cx + bw/2)), int(min(h-1, cy + bh/2))
    return nx1, ny1, nx2, ny2

def run_blip_question(pil_img: Image.Image, question: str) -> str:
    inputs = blip_proc(images=pil_img, text=question, return_tensors="pt").to(DEVICE)
    with torch.inference_mode():
        with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
            out = blip_model.generate(**inputs, max_new_tokens=BLIP_MAX_NEW_TOKENS)
    ans = blip_proc.tokenizer.decode(out[0], skip_special_tokens=True)
    # Thu d·ªçn VRAM
    del inputs, out
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
    gc.collect()
    return ans.strip()

def looks_suspicious(text: str) -> bool:
    low = text.lower()
    return any(k in low for k in SUS_KEYWORDS)

# ========================
# üß† PH√ÇN T√çCH FRAME: YOLO + BLIP tr√™n ROI nghi ng·ªù
# ========================
def analyze_frame_fused(frame_bgr):
    """
    - YOLO ph√°t hi·ªán person/phone/cheating
    - N·∫øu c√≥ 'cheating' ‚Üí x√°c nh·∫≠n ngay
    - N·∫øu c√≥ person & phone g·∫ßn nhau ‚Üí BLIP h·ªèi x√°c minh tr√™n ROI gh√©p
    """
    h, w, _ = frame_bgr.shape
    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)

    # YOLO predict
    results = yolo.predict(source=rgb, conf=CONF_DET, verbose=False)[0]
    if results is None or results.boxes is None or len(results.boxes) == 0:
        return frame_bgr, False, None

    boxes = []
    persons = []
    phones = []
    cheating_boxes = []

    # Thu th·∫≠p bbox theo nh√£n
    for b in results.boxes:
        cls_id = int(b.cls[0])
        label = yolo.names[cls_id] if hasattr(yolo, "names") else str(cls_id)
        conf = float(b.conf[0])
        x1, y1, x2, y2 = map(int, b.xyxy[0])
        boxes.append((label, conf, (x1, y1, x2, y2)))

        if label.lower() in PERSON_LABELS:
            persons.append((conf, (x1, y1, x2, y2)))
        if label.lower() in PHONE_LABELS:
            phones.append((conf, (x1, y1, x2, y2)))
        if label.lower() in CHEAT_LABELS:
            cheating_boxes.append((conf, (x1, y1, x2, y2)))

    # V·∫Ω t·∫•t c·∫£ bbox
    for label, conf, (x1, y1, x2, y2) in boxes:
        color = (0, 255, 0)
        if label.lower() in PHONE_LABELS: color = (255, 255, 0)
        if label.lower() in CHEAT_LABELS: color = (0, 0, 255)
        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)
        cv2.putText(frame_bgr, f"{label} {conf:.2f}", (x1, max(0, y1-6)),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, color, 2)

    # N·∫øu YOLO ƒë√£ c√≥ 'cheating' ‚Üí x√°c minh nh·∫π b·∫±ng BLIP (to√†n ROI) ƒë·ªÉ l·∫•y m√¥ t·∫£
    if cheating_boxes:
        # L·∫•y box c√≥ conf cao nh·∫•t
        cheating_boxes.sort(key=lambda x: x[0], reverse=True)
        _, (x1, y1, x2, y2) = cheating_boxes[0]
        rx1, ry1, rx2, ry2 = expand_box(x1, y1, x2, y2, 1.1, w, h)
        roi = rgb[ry1:ry2, rx1:rx2]
        pil = Image.fromarray(roi)
        caption = run_blip_question(pil, "Describe the suspicious behavior in this image in one sentence.")
        return frame_bgr, True, caption

    # N·∫øu kh√¥ng c√≥ 'cheating', nh∆∞ng c√≥ person + phone ‚Üí ki·ªÉm tra g·∫ßn nhau r·ªìi BLIP h·ªèi
    suspicious_caption = None
    S_MAX_DIST_PIX = max(40, int(0.08 * max(w, h)))  # ng∆∞·ª°ng g·∫ßn nhau (t∆∞∆°ng ƒë·ªëi theo k√≠ch th∆∞·ªõc ·∫£nh)
    for p_conf, (px1, py1, px2, py2) in persons:
        pcx, pcy = (px1+px2)//2, (py1+py2)//2
        for ph_conf, (hx1, hy1, hx2, hy2) in phones:
            hcx, hcy = (hx1+hx2)//2, (hy1+hy2)//2
            dist = math.hypot(pcx - hcx, pcy - hcy)
            if dist <= S_MAX_DIST_PIX:
                # G·ªôp ROI person + phone
                gx1, gy1 = min(px1, hx1), min(py1, hy1)
                gx2, gy2 = max(px2, hx2), max(py2, hy2)
                gx1, gy1, gx2, gy2 = expand_box(gx1, gy1, gx2, gy2, 1.1, w, h)
                roi = rgb[gy1:gy2, gx1:gx2]
                pil = Image.fromarray(roi)
                ans = run_blip_question(
                    pil,
                    "Is the student cheating in the exam? Answer briefly (e.g., 'using a phone', 'copying', or 'no')."
                )
                if looks_suspicious(ans):
                    suspicious_caption = ans
                    # Vi·ªÅn ƒë·ªè v√πng nghi ng·ªù
                    cv2.rectangle(frame_bgr, (gx1, gy1), (gx2, gy2), (0, 0, 255), 2)
                    cv2.putText(frame_bgr, "suspicious", (gx1, max(0, gy1-6)),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,255), 2)
                    return frame_bgr, True, suspicious_caption

    return frame_bgr, False, None

# ========================
# üé• CAMERA LOOP + C·∫¢NH B√ÅO
# ========================
current_frame_bgr = None
last_alert_ts = 0.0
ALERT_COOLDOWN = 2.0  # gi√¢y, tr√°nh k√™u chu√¥ng li√™n t·ª•c

def save_evidence(frame_bgr, caption: str | None):
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    img_path = os.path.join(EVIDENCE_DIR, f"cheating_{ts}.jpg")
    cv2.imwrite(img_path, frame_bgr)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ALERT -> {caption or 'cheating detected'} | IMG: {img_path}\n")
    print(f"üíæ L∆∞u b·∫±ng ch·ª©ng: {img_path} | M√¥ t·∫£: {caption}")

def camera_loop():
    global current_frame_bgr, last_alert_ts
    cap = cv2.VideoCapture(CAM_URL)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

    if not cap.isOpened():
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi camera. Ki·ªÉm tra l·∫°i IP Webcam.")
        return
    print("üì∑ Camera ƒë√£ k·∫øt n·ªëi th√†nh c√¥ng!")

    while True:
        ok, frame = cap.read()
        if not ok:
            continue
        frame = cv2.resize(frame, (640, 480))
        current_frame_bgr = frame.copy()

        # Ph√¢n t√≠ch frame (YOLO + BLIP ROI)
        analyzed, is_cheat, caption = analyze_frame_fused(frame)

        # C·∫£nh b√°o
        now = time.time()
        if is_cheat and (now - last_alert_ts >= ALERT_COOLDOWN):
            beep()
            save_evidence(analyzed, caption)
            last_alert_ts = now

        cv2.imshow("üì° Gi√°m s√°t thi c·ª≠ (Realtime) ‚Äî nh·∫•n 'q' ƒë·ªÉ tho√°t", analyzed)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()

# ========================
# üí¨ BLIP-2 Q&A TR√äN KHUNG H√åNH HI·ªÜN T·∫†I
# ========================
def chat_with_vlm(message, history):
    global current_frame_bgr
    if current_frame_bgr is None:
        return {"role": "assistant", "content": "‚ö†Ô∏è Ch∆∞a c√≥ khung h√¨nh camera."}

    # H·ªèi tr√™n to√†n khung h√¨nh hi·ªán t·∫°i (ƒë·ªÉ ng∆∞·ªùi d√πng t·ª± do Q&A)
    rgb = cv2.cvtColor(current_frame_bgr, cv2.COLOR_BGR2RGB)
    pil = Image.fromarray(rgb)
    ans = run_blip_question(pil, message)
    return {"role": "assistant", "content": f"üß† {ans}"}

# ========================
# üåê GIAO DI·ªÜN GRADIO
# ========================
with gr.Blocks(theme="soft") as demo:
    gr.Markdown("## ü§ñ AI Gi√°m s√°t thi c·ª≠ ‚Äî YOLOv8 + BLIP-2 Flan-T5-XL")
    gr.Markdown(
        "‚Ä¢ C·ª≠a s·ªï OpenCV hi·ªÉn th·ªã camera realtime\n"
        "‚Ä¢ T·ª± ƒë·ªông c·∫£nh b√°o + l∆∞u b·∫±ng ch·ª©ng khi x√°c nh·∫≠n gian l·∫≠n (YOLO + BLIP-2)\n"
        "‚Ä¢ H·ªèi-ƒë√°p v·ªÅ khung h√¨nh hi·ªán t·∫°i ·ªü khung chat b√™n d∆∞·ªõi"
    )
    gr.ChatInterface(
        fn=chat_with_vlm,
        title="BLIP-2 Q&A (khung h√¨nh hi·ªán t·∫°i)",
        textbox=gr.Textbox(placeholder="V√≠ d·ª•: 'Is anyone using a phone?'", lines=1),
        type="messages",
    )

# ========================
# üöÄ CH·∫†Y SONG SONG
# ========================
if __name__ == "__main__":
    threading.Thread(target=camera_loop, daemon=True).start()
    demo.launch(share=False)
