import os
from pathlib import Path
import torch
import cv2
import numpy as np
import gradio as gr
from ultralytics import YOLO
from deep_sort_realtime.deepsort_tracker import DeepSort
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
from datetime import datetime

# ======================
# ğŸ”§ Cáº¥u hÃ¬nh cÆ¡ báº£n
# ======================
# Ã‰p thÆ° má»¥c lÃ m viá»‡c vá» gá»‘c dá»± Ã¡n
os.chdir(r"D:\DaihocDaiNam\Nam4\Ki1\Chuyen_doi_so\Baitaplon\exam_monitoring_vlm")
print("ğŸ“‚ ÄÃ£ chuyá»ƒn thÆ° má»¥c lÃ m viá»‡c vá»:", os.getcwd())

# ÄÆ°á»ng dáº«n model YOLO tuyá»‡t Ä‘á»‘i
weights_path = Path(
    r"D:\DaihocDaiNam\Nam4\Ki1\Chuyen_doi_so\Baitaplon\exam_monitoring_vlm\runs\detect\train_rtx3050\weights\best.pt"
).resolve(strict=True)
print("ğŸ”’ Load model tá»«:", weights_path)
print("ğŸ“„ File tá»“n táº¡i:", os.path.exists(weights_path))

# ======================
# ğŸ§  Load YOLO model
# ======================
print("ğŸ§© Äang load model trá»±c tiáº¿p qua torch...")
model = YOLO(str(weights_path))  # DÃ¹ng Ultralytics API chÃ­nh thá»‘ng
print("âœ… Model YOLO load thÃ nh cÃ´ng!")

# ======================
# ğŸ§  Load CLIP model
# ======================
print("ğŸ§  Äang táº£i mÃ´ hÃ¬nh CLIP...")
clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
clip_proc = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
print("âœ… CLIP model táº£i thÃ nh cÃ´ng!")

# ======================
# ğŸš¶ DeepSort tracker
# ======================
tracker = DeepSort(max_age=30)

# ======================
# âš™ï¸ Cáº¥u hÃ¬nh ngÆ°á»¡ng vÃ  nhÃ£n
# ======================
CONF_THRESH = 0.15
EVENT_PHONE_DISTANCE_PIX = 80
CLIP_CONF_THRESH = 0.6

CLIP_LABELS = [
    "student using phone during exam",
    "student holding phone but not using it",
    "student looking at neighbor's paper",
    "student writing on paper normally"
]

# ======================
# ğŸ§  HÃ m tÃ­nh Ä‘iá»ƒm CLIP
# ======================
def clip_score(image_crop):
    inputs = clip_proc(text=CLIP_LABELS, images=image_crop, return_tensors="pt", padding=True)
    outputs = clip_model(**inputs)
    logits = outputs.logits_per_image[0]
    probs = logits.softmax(dim=0).detach().cpu().numpy()
    idx = probs.argmax()
    return CLIP_LABELS[idx], float(probs[idx])

# ======================
# ğŸ–¼ï¸ HÃ m xá»­ lÃ½ khung hÃ¬nh
# ======================
def process_frame(frame):
    if frame is None:
        return None

    results = model(frame, verbose=False)[0]
    print("ğŸ“¦ Tá»•ng sá»‘ box detect:", len(results.boxes))

    dets = []
    for box in results.boxes:
        conf = float(box.conf[0])
        if conf < CONF_THRESH:
            continue
        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
        cls_id = int(box.cls[0])
        label = model.names.get(cls_id, f"id_{cls_id}")

        # âœ… In ra log Ä‘á»ƒ kiá»ƒm tra model cÃ³ nháº­n Ä‘Ãºng class khÃ´ng
        print(f"ğŸ¯ PhÃ¡t hiá»‡n: {label} ({conf:.2f})")

        # Váº½ mÃ u theo class
        color = (0, 0, 255) if label == "cheating" else (0, 255, 0)

        # Váº½ khung
        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)
        cv2.putText(frame, f"{label} {conf:.2f}", (x1, y1 - 5),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)

    return frame

# ======================
# ğŸŒ Giao diá»‡n Gradio
# ======================
demo = gr.Interface(
    fn=process_frame,
    inputs=gr.Image(type="numpy", label="Upload Image or Frame"),  # âŒ Bá» source="upload"
    outputs=gr.Image(type="numpy", label="Processed Frame"),
    title="ğŸ“· Exam Monitoring with YOLO + CLIP + DeepSort",
    description="PhÃ¡t hiá»‡n gian láº­n thi cá»­ báº±ng camera real-time (YOLOv8 + DeepSort + CLIP)"
)

# ======================
# ğŸš€ Cháº¡y á»©ng dá»¥ng
# ======================
if __name__ == "__main__":
    demo.launch()
